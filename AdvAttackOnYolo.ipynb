{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/miladlink/TinyYoloV2\n",
    "\n",
    "https://github.com/eriklindernoren/PyTorch-YOLOv3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WVDh0TibRADG"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "# import skimage.io as io\n",
    "# import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets.coco import CocoDetection\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.YOLOv2 import *\n",
    "from models.YOLOv3 import load_model\n",
    "from attacks.FGSM import FGSM\n",
    "from attacks.PGD import PGD\n",
    "from attacks.CW import CW\n",
    "from attacks.noise import Noise\n",
    "from detect import detect_image\n",
    "from utils.loss import compute_loss\n",
    "from utils.utils import load_classes, rescale_boxes, non_max_suppression, print_environment_info\n",
    "from utils.augmentations import TRANSFORM_TRAIN, TRANSFORM_VAL\n",
    "from utils.transforms import DEFAULT_TRANSFORMS, Resize, ResizeEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jlQVknSfeKt4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelv = 3\n",
    "img_size=416\n",
    "\n",
    "if modelv == 2:\n",
    "    model = load_model_v2(weights = './weights/yolov2-tiny-voc.weights').to(device)\n",
    "    class_names = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'TVmonitor'] \n",
    "    root_train = \"./data/VOC2007/JPEGImages\"\n",
    "    annFile_train = \"./data/VOC2007/annotations/train.json\"\n",
    "    root_val = \"./data/VOC2007/JPEGImages\"\n",
    "    annFile_val = \"./data/VOC2007/annotations/val.json\"\n",
    "    \n",
    "elif modelv == 3:\n",
    "    model = load_model(\"./config/yolov3.cfg\", \"./weights/yolov3.weights\")\n",
    "    class_names = ['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "    id_list = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90])\n",
    "    root_train = \"./data/COCO2017/images/train_sample\"\n",
    "    annFile_train = \"./data/COCO2017/annotations/instances_train2017_modified_sample.json\"\n",
    "    root_val = \"./data/COCO2017/images/valid_sample\"\n",
    "    annFile_val = \"./data/COCO2017/annotations/instances_val2017_modified_sample.json\"\n",
    "    \n",
    "else:\n",
    "    print(\"invalid model number!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyxy2xywh(x):\n",
    "    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\n",
    "    y[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\n",
    "    y[..., 2] = x[..., 2] - x[..., 0]  # width\n",
    "    y[..., 3] = x[..., 3] - x[..., 1]  # height\n",
    "    return y\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "def yolo2json(boxes, img_copy, image_id):\n",
    "    # * put into coco format of x_min,y_min, width, height, bbox_conf, cls\n",
    "    # yolo format is x_center, y_center, w, h, bbox_conf, cls_conf, cls\n",
    "    predictions = []\n",
    "    for box in boxes:\n",
    "        x_center, y_center, w, h, conf, cls = box\n",
    "        x_min = max(0, (x_center - w / 2) * img_copy.shape[3])\n",
    "        y_min = max(0, (y_center - h / 2) * img_copy.shape[2])\n",
    "        width = min(img_copy.shape[3], w * img_copy.shape[3])\n",
    "        height = min(img_copy.shape[2], h * img_copy.shape[2])\n",
    "        # print(x_min,y_min, width, height, bbox_conf, cls)\n",
    "        predictions.append({\n",
    "            'image_id': image_id,\n",
    "            'category_id': int(id_list[int(cls)]) if modelv == 3 else int(cls),\n",
    "            'bbox': [int(x_min), int(y_min), int(width), int(height)],\n",
    "            'score': round(float(conf),2)\n",
    "        })\n",
    "    return predictions\n",
    "\n",
    "def nms2yolo(boxes, img_copy):\n",
    "    boxes = xyxy2xywh(boxes) # convert from coco to yolo: nms returns nx6 (x1, y1, x2, y2, conf, cls), change to center coordinates [x_center, y_center, width, height]\n",
    "    boxes[:,0] = boxes[:,0]/img_copy.shape[3]\n",
    "    boxes[:,1] = boxes[:,1]/img_copy.shape[2]\n",
    "    boxes[:,2] = boxes[:,2]/img_copy.shape[3]\n",
    "    boxes[:,3] = boxes[:,3]/img_copy.shape[2]\n",
    "    return boxes\n",
    "\n",
    "def saveImageWithBoxes(images, boxes, class_names, fileName):  \n",
    "    to_pil = transforms.ToPILImage() \n",
    "    pil_image = to_pil(images.squeeze())\n",
    "    pred_img = plot_boxes(pil_image, boxes, None, class_names)\n",
    "    pred_img.save(fileName)\n",
    "    \n",
    "def saveImage(img):\n",
    "    # * just for sanity check, output image. put the dim 3 at the back\n",
    "    imageN = img.clone().detach()\n",
    "    imageN = imageN.cpu().squeeze().permute(1, 2, 0).numpy() \n",
    "    imageN = cv2.cvtColor(imageN, cv2.COLOR_RGB2BGR)\n",
    "    # print(imageN.shape)\n",
    "    cv2.imwrite(\"data/results/mygraph.jpg\", imageN*255) \n",
    "    \n",
    "def getOneIter(dataloader):\n",
    "    images, annotations = next(iter(dataloader))\n",
    "    np.set_printoptions(linewidth=500)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    print(\"dataloader out\")\n",
    "    print(annotations[0].numpy())\n",
    "    \n",
    "\n",
    "def imgToGreyscale(img):\n",
    "    if img.shape[0] != 3:\n",
    "        raise ValueError(\"Input tensor must have shape [3, H, W].\")\n",
    "    grayscale = 0.299 * img[0] + 0.587 * img[1] + 0.114 * img[2]\n",
    "    grayscale_tensor = grayscale.unsqueeze(0).repeat(3, 1, 1)\n",
    "    return grayscale_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dataloader (make different train and val later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# coco_dataset_train = CocoDetection(root=root_train, annFile=annFile_train, transform=TRANSFORM_TRAIN_IMG, target_transform=TRANSFORM_TRAIN_TARGET)\n",
    "coco_dataset_val = CocoDetection(root=root_val, annFile=annFile_val, transforms=TRANSFORM_VAL)\n",
    "coco_dataset_eval = CocoDetection(root=root_val, annFile=annFile_val, transform=transforms.Compose([transforms.ToTensor(),]))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Create a DataLoader for your COCO dataset\n",
    "train_loader = DataLoader(coco_dataset_val, batch_size=4, shuffle=True, collate_fn=collate_fn) # multiple images per batch\n",
    "val_loader = DataLoader(coco_dataset_val, batch_size=1, shuffle=True, collate_fn=collate_fn) # one per batch\n",
    "cocoeval_loader = DataLoader(coco_dataset_eval, batch_size=1, shuffle=True, collate_fn=collate_fn) # original images without transformatios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader out\n",
      "[[872.          37.           0.65160937   0.26881249   0.03028126   0.02582812]\n",
      " [872.           1.           0.24103124   0.25730078   0.45617185   0.51460156]\n",
      " [872.           1.           0.26989063   0.28642187   0.41514064   0.57284374]\n",
      " [872.          40.           0.5900625    0.24570312   0.0897656    0.07153125]]\n"
     ]
    }
   ],
   "source": [
    "getOneIter(val_loader) # print targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attackImage = 0 # variable for saving attack image, run this first, change pruning ratio (attack), don't run this and only run below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.05\n",
    "# attacker = FGSM(model=model, epsilon=0.05)\n",
    "# attacker = PGD(model=model, epsilon=0.05, epoch=5, lr=0.02)\n",
    "attacker = CW(model=model, epsilon=eps, lr=eps/3, epoch=5, target=52) # 52 is banana\n",
    "# attacker = Noise(model=model, epsilon=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [00:00<00:00, 85.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "  Raw confidence range: -32.993 to 7.512\n",
      "  Sigmoid confidence range: 0.000 to 0.999\n",
      "  Predictions above 0.5: 7\n",
      "  Predictions above 0.1: 8\n",
      "Layer 1:\n",
      "  Raw confidence range: -34.192 to -4.186\n",
      "  Sigmoid confidence range: 0.000 to 0.015\n",
      "  Predictions above 0.5: 0\n",
      "  Predictions above 0.1: 0\n",
      "Layer 2:\n",
      "  Raw confidence range: -46.367 to -1.881\n",
      "  Sigmoid confidence range: 0.000 to 0.132\n",
      "  Predictions above 0.5: 0\n",
      "  Predictions above 0.1: 2\n",
      "Target shape: torch.Size([3, 4, 7])\n",
      "First few targets: tensor([[[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 0.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 1.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 1.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 1.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 2.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 2.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 2.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 2.0000e+00]]], device='cuda:0', dtype=torch.float64)\n",
      "Target format should be: (image_id, class, x, y, w, h)\n",
      "Target coordinates range: tensor(0., device='cuda:0', dtype=torch.float64) to tensor(40., device='cuda:0', dtype=torch.float64)\n",
      "Are coordinates normalized (0-1)? tensor(False, device='cuda:0')\n",
      "\n",
      "=== Layer 0 Anchor Diagnosis ===\n",
      "Anchors (scaled): tensor([[ 3.6250,  2.8125],\n",
      "        [ 4.8750,  6.1875],\n",
      "        [11.6562, 10.1875]], device='cuda:0')\n",
      "Target box sizes (sample): tensor([[0.3937, 0.3358],\n",
      "        [5.9302, 6.6898],\n",
      "        [5.3968, 7.4470],\n",
      "        [1.1670, 0.9299]], device='cuda:0', dtype=torch.float64)\n",
      "Max ratios range: 1.20 to 30.34\n",
      "Anchor 0: 3/4 targets match\n",
      "Anchor 1: 2/4 targets match\n",
      "Anchor 2: 2/4 targets match\n",
      "Targets matched in layer 0: 7 out of 12\n",
      "class tensor([ 1,  1, 40,  1,  1,  1,  1], device='cuda:0')\n",
      "Targets matched in layer 1: 5 out of 12\n",
      "class tensor([40, 40,  1,  1, 40], device='cuda:0')\n",
      "Targets matched in layer 2: 6 out of 12\n",
      "class tensor([37, 40, 37, 40, 37, 40], device='cuda:0')\n",
      "Layer 0:\n",
      "  Raw confidence range: -32.993 to 7.512\n",
      "  Sigmoid confidence range: 0.000 to 0.999\n",
      "  Predictions above 0.5: 7\n",
      "  Predictions above 0.1: 8\n",
      "Layer 1:\n",
      "  Raw confidence range: -34.192 to -4.186\n",
      "  Sigmoid confidence range: 0.000 to 0.015\n",
      "  Predictions above 0.5: 0\n",
      "  Predictions above 0.1: 0\n",
      "Layer 2:\n",
      "  Raw confidence range: -46.367 to -1.881\n",
      "  Sigmoid confidence range: 0.000 to 0.132\n",
      "  Predictions above 0.5: 0\n",
      "  Predictions above 0.1: 2\n",
      "Target shape: torch.Size([3, 4, 7])\n",
      "First few targets: tensor([[[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 0.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 1.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 1.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 1.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 2.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 2.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 2.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 2.0000e+00]]], device='cuda:0', dtype=torch.float64)\n",
      "Target format should be: (image_id, class, x, y, w, h)\n",
      "Target coordinates range: tensor(0., device='cuda:0', dtype=torch.float64) to tensor(40., device='cuda:0', dtype=torch.float64)\n",
      "Are coordinates normalized (0-1)? tensor(False, device='cuda:0')\n",
      "\n",
      "=== Layer 0 Anchor Diagnosis ===\n",
      "Anchors (scaled): tensor([[ 3.6250,  2.8125],\n",
      "        [ 4.8750,  6.1875],\n",
      "        [11.6562, 10.1875]], device='cuda:0')\n",
      "Target box sizes (sample): tensor([[0.3937, 0.3358],\n",
      "        [5.9302, 6.6898],\n",
      "        [5.3968, 7.4470],\n",
      "        [1.1670, 0.9299]], device='cuda:0', dtype=torch.float64)\n",
      "Max ratios range: 1.20 to 30.34\n",
      "Anchor 0: 3/4 targets match\n",
      "Anchor 1: 2/4 targets match\n",
      "Anchor 2: 2/4 targets match\n",
      "Targets matched in layer 0: 7 out of 12\n",
      "class tensor([ 1,  1, 40,  1,  1,  1,  1], device='cuda:0')\n",
      "Targets matched in layer 1: 5 out of 12\n",
      "class tensor([40, 40,  1,  1, 40], device='cuda:0')\n",
      "Targets matched in layer 2: 6 out of 12\n",
      "class tensor([37, 40, 37, 40, 37, 40], device='cuda:0')\n",
      "Layer 0:\n",
      "  Raw confidence range: -32.266 to 9.116\n",
      "  Sigmoid confidence range: 0.000 to 1.000\n",
      "  Predictions above 0.5: 7\n",
      "  Predictions above 0.1: 11\n",
      "Layer 1:\n",
      "  Raw confidence range: -38.499 to -0.933\n",
      "  Sigmoid confidence range: 0.000 to 0.282\n",
      "  Predictions above 0.5: 0\n",
      "  Predictions above 0.1: 2\n",
      "Layer 2:\n",
      "  Raw confidence range: -53.634 to -0.736\n",
      "  Sigmoid confidence range: 0.000 to 0.324\n",
      "  Predictions above 0.5: 0\n",
      "  Predictions above 0.1: 2\n",
      "Target shape: torch.Size([3, 4, 7])\n",
      "First few targets: tensor([[[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 0.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 1.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 1.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 1.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 2.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 2.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 2.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 2.0000e+00]]], device='cuda:0', dtype=torch.float64)\n",
      "Target format should be: (image_id, class, x, y, w, h)\n",
      "Target coordinates range: tensor(0., device='cuda:0', dtype=torch.float64) to tensor(40., device='cuda:0', dtype=torch.float64)\n",
      "Are coordinates normalized (0-1)? tensor(False, device='cuda:0')\n",
      "\n",
      "=== Layer 0 Anchor Diagnosis ===\n",
      "Anchors (scaled): tensor([[ 3.6250,  2.8125],\n",
      "        [ 4.8750,  6.1875],\n",
      "        [11.6562, 10.1875]], device='cuda:0')\n",
      "Target box sizes (sample): tensor([[0.3937, 0.3358],\n",
      "        [5.9302, 6.6898],\n",
      "        [5.3968, 7.4470],\n",
      "        [1.1670, 0.9299]], device='cuda:0', dtype=torch.float64)\n",
      "Max ratios range: 1.20 to 30.34\n",
      "Anchor 0: 3/4 targets match\n",
      "Anchor 1: 2/4 targets match\n",
      "Anchor 2: 2/4 targets match\n",
      "Targets matched in layer 0: 7 out of 12\n",
      "class tensor([ 1,  1, 40,  1,  1,  1,  1], device='cuda:0')\n",
      "Targets matched in layer 1: 5 out of 12\n",
      "class tensor([40, 40,  1,  1, 40], device='cuda:0')\n",
      "Targets matched in layer 2: 6 out of 12\n",
      "class tensor([37, 40, 37, 40, 37, 40], device='cuda:0')\n",
      "Layer 0:\n",
      "  Raw confidence range: -33.468 to 10.325\n",
      "  Sigmoid confidence range: 0.000 to 1.000\n",
      "  Predictions above 0.5: 8\n",
      "  Predictions above 0.1: 12\n",
      "Layer 1:\n",
      "  Raw confidence range: -44.872 to 0.374\n",
      "  Sigmoid confidence range: 0.000 to 0.592\n",
      "  Predictions above 0.5: 1\n",
      "  Predictions above 0.1: 4\n",
      "Layer 2:\n",
      "  Raw confidence range: -43.443 to 1.840\n",
      "  Sigmoid confidence range: 0.000 to 0.863\n",
      "  Predictions above 0.5: 3\n",
      "  Predictions above 0.1: 7\n",
      "Target shape: torch.Size([3, 4, 7])\n",
      "First few targets: tensor([[[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 0.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 1.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 1.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 1.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 2.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 2.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 2.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 2.0000e+00]]], device='cuda:0', dtype=torch.float64)\n",
      "Target format should be: (image_id, class, x, y, w, h)\n",
      "Target coordinates range: tensor(0., device='cuda:0', dtype=torch.float64) to tensor(40., device='cuda:0', dtype=torch.float64)\n",
      "Are coordinates normalized (0-1)? tensor(False, device='cuda:0')\n",
      "\n",
      "=== Layer 0 Anchor Diagnosis ===\n",
      "Anchors (scaled): tensor([[ 3.6250,  2.8125],\n",
      "        [ 4.8750,  6.1875],\n",
      "        [11.6562, 10.1875]], device='cuda:0')\n",
      "Target box sizes (sample): tensor([[0.3937, 0.3358],\n",
      "        [5.9302, 6.6898],\n",
      "        [5.3968, 7.4470],\n",
      "        [1.1670, 0.9299]], device='cuda:0', dtype=torch.float64)\n",
      "Max ratios range: 1.20 to 30.34\n",
      "Anchor 0: 3/4 targets match\n",
      "Anchor 1: 2/4 targets match\n",
      "Anchor 2: 2/4 targets match\n",
      "Targets matched in layer 0: 7 out of 12\n",
      "class tensor([ 1,  1, 40,  1,  1,  1,  1], device='cuda:0')\n",
      "Targets matched in layer 1: 5 out of 12\n",
      "class tensor([40, 40,  1,  1, 40], device='cuda:0')\n",
      "Targets matched in layer 2: 6 out of 12\n",
      "class tensor([37, 40, 37, 40, 37, 40], device='cuda:0')\n",
      "Layer 0:\n",
      "  Raw confidence range: -35.623 to 11.123\n",
      "  Sigmoid confidence range: 0.000 to 1.000\n",
      "  Predictions above 0.5: 9\n",
      "  Predictions above 0.1: 9\n",
      "Layer 1:\n",
      "  Raw confidence range: -52.635 to 0.520\n",
      "  Sigmoid confidence range: 0.000 to 0.627\n",
      "  Predictions above 0.5: 1\n",
      "  Predictions above 0.1: 3\n",
      "Layer 2:\n",
      "  Raw confidence range: -51.248 to -0.431\n",
      "  Sigmoid confidence range: 0.000 to 0.394\n",
      "  Predictions above 0.5: 0\n",
      "  Predictions above 0.1: 2\n",
      "Target shape: torch.Size([3, 4, 7])\n",
      "First few targets: tensor([[[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 0.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 1.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 1.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 1.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 2.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 2.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 2.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 2.0000e+00]]], device='cuda:0', dtype=torch.float64)\n",
      "Target format should be: (image_id, class, x, y, w, h)\n",
      "Target coordinates range: tensor(0., device='cuda:0', dtype=torch.float64) to tensor(40., device='cuda:0', dtype=torch.float64)\n",
      "Are coordinates normalized (0-1)? tensor(False, device='cuda:0')\n",
      "\n",
      "=== Layer 0 Anchor Diagnosis ===\n",
      "Anchors (scaled): tensor([[ 3.6250,  2.8125],\n",
      "        [ 4.8750,  6.1875],\n",
      "        [11.6562, 10.1875]], device='cuda:0')\n",
      "Target box sizes (sample): tensor([[0.3937, 0.3358],\n",
      "        [5.9302, 6.6898],\n",
      "        [5.3968, 7.4470],\n",
      "        [1.1670, 0.9299]], device='cuda:0', dtype=torch.float64)\n",
      "Max ratios range: 1.20 to 30.34\n",
      "Anchor 0: 3/4 targets match\n",
      "Anchor 1: 2/4 targets match\n",
      "Anchor 2: 2/4 targets match\n",
      "Targets matched in layer 0: 7 out of 12\n",
      "class tensor([ 1,  1, 40,  1,  1,  1,  1], device='cuda:0')\n",
      "Targets matched in layer 1: 5 out of 12\n",
      "class tensor([40, 40,  1,  1, 40], device='cuda:0')\n",
      "Targets matched in layer 2: 6 out of 12\n",
      "class tensor([37, 40, 37, 40, 37, 40], device='cuda:0')\n",
      "Layer 0:\n",
      "  Raw confidence range: -38.362 to 10.378\n",
      "  Sigmoid confidence range: 0.000 to 1.000\n",
      "  Predictions above 0.5: 9\n",
      "  Predictions above 0.1: 11\n",
      "Layer 1:\n",
      "  Raw confidence range: -52.493 to 0.033\n",
      "  Sigmoid confidence range: 0.000 to 0.508\n",
      "  Predictions above 0.5: 1\n",
      "  Predictions above 0.1: 1\n",
      "Layer 2:\n",
      "  Raw confidence range: -46.486 to 4.328\n",
      "  Sigmoid confidence range: 0.000 to 0.987\n",
      "  Predictions above 0.5: 2\n",
      "  Predictions above 0.1: 7\n",
      "Target shape: torch.Size([3, 4, 7])\n",
      "First few targets: tensor([[[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 0.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 1.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 1.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 1.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 2.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 2.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 2.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 2.0000e+00]]], device='cuda:0', dtype=torch.float64)\n",
      "Target format should be: (image_id, class, x, y, w, h)\n",
      "Target coordinates range: tensor(0., device='cuda:0', dtype=torch.float64) to tensor(40., device='cuda:0', dtype=torch.float64)\n",
      "Are coordinates normalized (0-1)? tensor(False, device='cuda:0')\n",
      "\n",
      "=== Layer 0 Anchor Diagnosis ===\n",
      "Anchors (scaled): tensor([[ 3.6250,  2.8125],\n",
      "        [ 4.8750,  6.1875],\n",
      "        [11.6562, 10.1875]], device='cuda:0')\n",
      "Target box sizes (sample): tensor([[0.3937, 0.3358],\n",
      "        [5.9302, 6.6898],\n",
      "        [5.3968, 7.4470],\n",
      "        [1.1670, 0.9299]], device='cuda:0', dtype=torch.float64)\n",
      "Max ratios range: 1.20 to 30.34\n",
      "Anchor 0: 3/4 targets match\n",
      "Anchor 1: 2/4 targets match\n",
      "Anchor 2: 2/4 targets match\n",
      "Targets matched in layer 0: 7 out of 12\n",
      "class tensor([ 1,  1, 40,  1,  1,  1,  1], device='cuda:0')\n",
      "Targets matched in layer 1: 5 out of 12\n",
      "class tensor([40, 40,  1,  1, 40], device='cuda:0')\n",
      "Targets matched in layer 2: 6 out of 12\n",
      "class tensor([37, 40, 37, 40, 37, 40], device='cuda:0')\n",
      "Layer 0:\n",
      "  Raw confidence range: -40.131 to 9.741\n",
      "  Sigmoid confidence range: 0.000 to 1.000\n",
      "  Predictions above 0.5: 10\n",
      "  Predictions above 0.1: 10\n",
      "Layer 1:\n",
      "  Raw confidence range: -64.593 to -0.396\n",
      "  Sigmoid confidence range: 0.000 to 0.402\n",
      "  Predictions above 0.5: 0\n",
      "  Predictions above 0.1: 5\n",
      "Layer 2:\n",
      "  Raw confidence range: -53.260 to -0.437\n",
      "  Sigmoid confidence range: 0.000 to 0.392\n",
      "  Predictions above 0.5: 0\n",
      "  Predictions above 0.1: 3\n",
      "Target shape: torch.Size([3, 4, 7])\n",
      "First few targets: tensor([[[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 0.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 1.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 1.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 1.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 3.7000e+01, 6.5161e-01, 2.6881e-01, 3.0281e-02,\n",
      "          2.5828e-02, 2.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.4103e-01, 2.5730e-01, 4.5617e-01,\n",
      "          5.1460e-01, 2.0000e+00],\n",
      "         [0.0000e+00, 1.0000e+00, 2.6989e-01, 2.8642e-01, 4.1514e-01,\n",
      "          5.7284e-01, 2.0000e+00],\n",
      "         [0.0000e+00, 4.0000e+01, 5.9006e-01, 2.4570e-01, 8.9766e-02,\n",
      "          7.1531e-02, 2.0000e+00]]], device='cuda:0', dtype=torch.float64)\n",
      "Target format should be: (image_id, class, x, y, w, h)\n",
      "Target coordinates range: tensor(0., device='cuda:0', dtype=torch.float64) to tensor(40., device='cuda:0', dtype=torch.float64)\n",
      "Are coordinates normalized (0-1)? tensor(False, device='cuda:0')\n",
      "\n",
      "=== Layer 0 Anchor Diagnosis ===\n",
      "Anchors (scaled): tensor([[ 3.6250,  2.8125],\n",
      "        [ 4.8750,  6.1875],\n",
      "        [11.6562, 10.1875]], device='cuda:0')\n",
      "Target box sizes (sample): tensor([[0.3937, 0.3358],\n",
      "        [5.9302, 6.6898],\n",
      "        [5.3968, 7.4470],\n",
      "        [1.1670, 0.9299]], device='cuda:0', dtype=torch.float64)\n",
      "Max ratios range: 1.20 to 30.34\n",
      "Anchor 0: 3/4 targets match\n",
      "Anchor 1: 2/4 targets match\n",
      "Anchor 2: 2/4 targets match\n",
      "Targets matched in layer 0: 7 out of 12\n",
      "class tensor([ 1,  1, 40,  1,  1,  1,  1], device='cuda:0')\n",
      "Targets matched in layer 1: 5 out of 12\n",
      "class tensor([40, 40,  1,  1, 40], device='cuda:0')\n",
      "Targets matched in layer 2: 6 out of 12\n",
      "class tensor([37, 40, 37, 40, 37, 40], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.4497121   0.5275377   0.46953103  0.6969084   0.99979293  0.        ]\n",
      " [ 0.580508    0.32225165  0.21504769  0.28952548  0.5018697   0.        ]\n",
      " [ 0.590916    0.23952095  0.06103281  0.13069054  0.43250215 34.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "predictionsBefore = []\n",
    "predictionsAfter = []\n",
    "lossesBefore = []\n",
    "lossesAfter = []\n",
    "mode = \"image\" # need different modes if i want to save image or output prediction json\n",
    "# mode = \"json\"\n",
    "# image_ids= [71711,19221,22192] # output images that i want, 19221 is broccoli, 22192 is dog, 71711 is plane\n",
    "image_ids= [872] # sample image id\n",
    "\n",
    "os.makedirs(\"./data/results/images\", exist_ok=True)\n",
    "\n",
    "for i, (images, targets) in enumerate(tqdm(val_loader)):\n",
    "    if targets[0].numel() != 0:\n",
    "        with torch.no_grad():\n",
    "            #* modify inputs to be in proper shape\n",
    "            images = torch.stack(images) # images.shape is [n, 3, 416, 416] (even if n=1)\n",
    "            images = images.to(device)\n",
    "            image_id = int(targets[0][0,0].cpu().numpy()) # assume 1 image\n",
    "            if image_id not in image_ids: continue # for when we want outputs of specific images\n",
    "            for i, boxes in enumerate(targets): # targets is nx6, (image,class,x,y,w,h)\n",
    "                if boxes.ndim == 2: boxes[:, 0] = i # change out image_id to id in batch to conform to compute_loss. this is normally done in ListDataset -> collate_fn. the id now starts at 0 for each image\n",
    "            targets = torch.cat(targets, 0).to(device) # from tuples to one tensor\n",
    "            originalImageSize = targets[0, 6:].cpu().numpy() # original image shape, assume one image per batch\n",
    "            targets = targets[:, :6]\n",
    "            \n",
    "            #* loss\n",
    "            model.train()\n",
    "            # start = time.time()\n",
    "            outputsBefore = model(images)\n",
    "            # end = time.time()\n",
    "            # print(end - start)\n",
    "            lossBefore, loss_components = compute_loss(outputsBefore, targets, model)\n",
    "            lossesBefore.append(lossBefore.cpu().numpy())\n",
    "            \n",
    "            images_adv = attacker.forward(images, targets) # get adversarial image\n",
    "            \n",
    "            outputsAfter = model(images_adv)\n",
    "            lossAfter, loss_components = compute_loss(outputsAfter, targets, model)\n",
    "            lossesAfter.append(lossAfter.cpu().numpy())\n",
    "            \n",
    "            #* plot\n",
    "            model.eval()\n",
    "            \n",
    "            # ground truth\n",
    "            # print(targets) #(ima ge,class,x,y,w,h), the class id starts from 1\n",
    "            # nms is (x1, y1, x2, y2, conf, cls), the class id starts from 0\n",
    "            # yolo is (x_center, y_center, width, height, conf. cls)\n",
    "            \n",
    "            # before attack\n",
    "            outputsBefore = model(images[0].unsqueeze(0))\n",
    "            boxesBefore = non_max_suppression(outputsBefore, conf_thres=0.3, iou_thres=0.5)[0].numpy()\n",
    "            if mode == \"json\":\n",
    "                boxesBefore = rescale_boxes(boxesBefore, img_size, originalImageSize)\n",
    "            boxesBefore = nms2yolo(boxesBefore, images)\n",
    "            if mode == \"image\":\n",
    "                saveImageWithBoxes(images[0], boxesBefore, class_names, f\"./data/results/images/attack_before_{image_id}.jpg\")\n",
    "            if mode == \"json\":\n",
    "                predictionsBefore += yolo2json(boxesBefore, images[0].unsqueeze(0), image_id)\n",
    "                \n",
    "            # after attack\n",
    "            outputsAfter = model(images_adv[0].unsqueeze(0))\n",
    "            boxesAfter = non_max_suppression(outputsAfter, conf_thres=0.3, iou_thres=0.5)[0].numpy()\n",
    "            \n",
    "            \n",
    "            if mode == \"json\":\n",
    "                boxesAfter = rescale_boxes(boxesAfter, img_size, originalImageSize)\n",
    "            # print(boxesAfter)\n",
    "            boxesAfter = nms2yolo(boxesAfter, images_adv)\n",
    "            print(boxesAfter)\n",
    "            if mode == \"image\":\n",
    "                saveImageWithBoxes(images_adv[0], boxesAfter, class_names, f\"./data/results/images/attack_after_{image_id}.jpg\")\n",
    "                \n",
    "                # attackImage = images_adv[0] # for saving the same attack image for different pruning ratios, comment out after save\n",
    "                # saveImageWithBoxes(attackImage, boxesAfter, class_names, f\"./data/results/images/pruning/{image_id}/attack_after_99_x.jpg\") # plot different pruning ratios with same attack image\n",
    "                \n",
    "                # greyscaleAttackImage = imgToGreyscale(attackImage)\n",
    "                # saveImageWithBoxes(greyscaleAttackImage, boxesAfter, class_names, f\"./data/results/images/pruning/{image_id}/attack_after_x_grey.jpg\") # plot different pruning ratios with same attack image\n",
    "            if mode == \"json\":\n",
    "                predictionsAfter += yolo2json(boxesAfter, images_adv[0].unsqueeze(0), image_id)\n",
    "            # time.sleep(0.1) # for using noise attack\n",
    "            \n",
    "    else: continue # pics without targets\n",
    "    # break\n",
    "\n",
    "\n",
    "with open(f'./data/results/predictionsBefore.json', 'w') as f:\n",
    "    json.dump(predictionsBefore, f)\n",
    "with open(f'./data/results/predictionsAfter.json', 'w') as f:\n",
    "    json.dump(predictionsAfter, f)\n",
    "np.savetxt(\"./data/results/lossesBefore.csv\", lossesBefore, delimiter=\",\")\n",
    "np.savetxt(\"./data/results/lossesAfter.csv\", lossesAfter, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss before attack: 0.37322962284088135\n",
      "Avg loss after attack: 0.8218954801559448\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('./data/results/lossesBefore.csv', delimiter=',')\n",
    "average = np.mean(data)\n",
    "print(\"Avg loss before attack:\", average)\n",
    "data = np.loadtxt('./data/results/lossesAfter.csv', delimiter=',')\n",
    "average = np.mean(data)\n",
    "print(\"Avg loss after attack:\", average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "  Raw confidence range: -41.378 to 7.641\n",
      "  Sigmoid confidence range: 0.000 to 1.000\n",
      "  Predictions above 0.5: 27\n",
      "  Predictions above 0.1: 47\n",
      "Layer 1:\n",
      "  Raw confidence range: -43.884 to 6.114\n",
      "  Sigmoid confidence range: 0.000 to 0.998\n",
      "  Predictions above 0.5: 14\n",
      "  Predictions above 0.1: 29\n",
      "Layer 2:\n",
      "  Raw confidence range: -54.977 to 6.626\n",
      "  Sigmoid confidence range: 0.000 to 0.999\n",
      "  Predictions above 0.5: 7\n",
      "  Predictions above 0.1: 38\n",
      "Target shape: torch.Size([3, 31, 7])\n",
      "First few targets: tensor([[[0.0000e+00, 7.3000e+01, 9.9016e-02, 4.3681e-01, 1.9803e-01,\n",
      "          4.2472e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 7.4000e+01, 3.7800e-01, 6.8116e-01, 1.1803e-01,\n",
      "          6.8937e-02, 0.0000e+00],\n",
      "         [0.0000e+00, 7.6000e+01, 5.0353e-01, 6.0137e-01, 4.8222e-01,\n",
      "          1.4225e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 7.2000e+01, 3.9269e-01, 1.6081e-01, 3.4887e-01,\n",
      "          2.7978e-01, 0.0000e+00],\n",
      "         [0.0000e+00, 7.4000e+01, 9.5484e-01, 6.0747e-01, 4.5000e-02,\n",
      "          3.0188e-02, 0.0000e+00],\n",
      "         [1.0000e+00, 5.4000e+01, 1.9731e-01, 3.8900e-01, 3.9462e-01,\n",
      "          3.4081e-01, 0.0000e+00],\n",
      "         [1.0000e+00, 5.1000e+01, 7.6053e-01, 3.8520e-01, 2.3947e-01,\n",
      "          3.1391e-01, 0.0000e+00],\n",
      "         [2.0000e+00, 6.4000e+01, 3.7028e-01, 3.8986e-01, 3.8594e-02,\n",
      "          1.0859e-01, 0.0000e+00],\n",
      "         [2.0000e+00, 7.2000e+01, 6.3820e-02, 4.2931e-01, 1.2764e-01,\n",
      "          1.4823e-01, 0.0000e+00],\n",
      "         [2.0000e+00, 7.2000e+01, 8.7064e-01, 4.9405e-01, 1.2711e-01,\n",
      "          1.2302e-01, 0.0000e+00],\n",
      "         [2.0000e+00, 6.2000e+01, 5.6091e-01, 5.0789e-01, 8.7500e-02,\n",
      "          1.6067e-01, 0.0000e+00],\n",
      "         [2.0000e+00, 6.2000e+01, 4.5420e-01, 5.0781e-01, 9.6609e-02,\n",
      "          1.5387e-01, 0.0000e+00],\n",
      "         [2.0000e+00, 6.2000e+01, 6.4562e-01, 5.1564e-01, 4.7141e-02,\n",
      "          1.2713e-01, 0.0000e+00],\n",
      "         [2.0000e+00, 6.2000e+01, 4.9594e-01, 5.0975e-01, 3.3719e-02,\n",
      "          1.8109e-02, 0.0000e+00],\n",
      "         [2.0000e+00, 1.0000e+00, 6.4500e-01, 4.1345e-01, 8.2891e-02,\n",
      "          2.1564e-01, 0.0000e+00],\n",
      "         [2.0000e+00, 1.0000e+00, 6.0067e-01, 4.3627e-01, 2.3625e-02,\n",
      "          5.5844e-02, 0.0000e+00],\n",
      "         [2.0000e+00, 7.8000e+01, 8.0034e-01, 4.8867e-01, 2.3031e-02,\n",
      "          2.4953e-02, 0.0000e+00],\n",
      "         [2.0000e+00, 8.2000e+01, 7.7047e-01, 4.3959e-01, 3.1703e-02,\n",
      "          1.6923e-01, 0.0000e+00],\n",
      "         [2.0000e+00, 8.4000e+01, 9.4495e-01, 6.4514e-01, 2.2406e-02,\n",
      "          7.1422e-02, 0.0000e+00],\n",
      "         [2.0000e+00, 8.4000e+01, 9.5819e-01, 6.4881e-01, 2.0125e-02,\n",
      "          7.2563e-02, 0.0000e+00],\n",
      "         [2.0000e+00, 8.5000e+01, 6.9964e-01, 3.5644e-01, 2.1828e-02,\n",
      "          3.4187e-02, 0.0000e+00],\n",
      "         [2.0000e+00, 8.6000e+01, 8.5791e-01, 6.5067e-01, 5.7313e-02,\n",
      "          1.4011e-01, 0.0000e+00],\n",
      "         [2.0000e+00, 8.6000e+01, 5.4806e-01, 4.9350e-01, 1.7766e-02,\n",
      "          3.5234e-02, 0.0000e+00],\n",
      "         [2.0000e+00, 6.2000e+01, 6.4414e-01, 5.0941e-01, 1.5047e-02,\n",
      "          1.9563e-02, 0.0000e+00],\n",
      "         [2.0000e+00, 8.6000e+01, 3.7694e-01, 4.7186e-01, 2.2219e-02,\n",
      "          2.7547e-02, 0.0000e+00],\n",
      "         [2.0000e+00, 8.6000e+01, 5.2623e-01, 4.7891e-01, 1.5203e-02,\n",
      "          2.6141e-02, 0.0000e+00],\n",
      "         [2.0000e+00, 6.7000e+01, 5.0189e-01, 5.2847e-01, 1.9619e-01,\n",
      "          1.3895e-01, 0.0000e+00],\n",
      "         [3.0000e+00, 8.8000e+01, 2.1851e-01, 2.2804e-01, 4.3702e-01,\n",
      "          4.5609e-01, 0.0000e+00],\n",
      "         [3.0000e+00, 8.8000e+01, 2.0894e-01, 4.3469e-01, 4.1788e-01,\n",
      "          5.5405e-01, 0.0000e+00],\n",
      "         [3.0000e+00, 8.8000e+01, 3.1394e-01, 2.1741e-01, 5.1911e-01,\n",
      "          4.3483e-01, 0.0000e+00],\n",
      "         [3.0000e+00, 6.5000e+01, 2.5056e-01, 2.5005e-01, 5.0112e-01,\n",
      "          5.0011e-01, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 7.3000e+01, 9.9016e-02, 4.3681e-01, 1.9803e-01,\n",
      "          4.2472e-01, 1.0000e+00],\n",
      "         [0.0000e+00, 7.4000e+01, 3.7800e-01, 6.8116e-01, 1.1803e-01,\n",
      "          6.8937e-02, 1.0000e+00],\n",
      "         [0.0000e+00, 7.6000e+01, 5.0353e-01, 6.0137e-01, 4.8222e-01,\n",
      "          1.4225e-01, 1.0000e+00],\n",
      "         [0.0000e+00, 7.2000e+01, 3.9269e-01, 1.6081e-01, 3.4887e-01,\n",
      "          2.7978e-01, 1.0000e+00],\n",
      "         [0.0000e+00, 7.4000e+01, 9.5484e-01, 6.0747e-01, 4.5000e-02,\n",
      "          3.0188e-02, 1.0000e+00],\n",
      "         [1.0000e+00, 5.4000e+01, 1.9731e-01, 3.8900e-01, 3.9462e-01,\n",
      "          3.4081e-01, 1.0000e+00],\n",
      "         [1.0000e+00, 5.1000e+01, 7.6053e-01, 3.8520e-01, 2.3947e-01,\n",
      "          3.1391e-01, 1.0000e+00],\n",
      "         [2.0000e+00, 6.4000e+01, 3.7028e-01, 3.8986e-01, 3.8594e-02,\n",
      "          1.0859e-01, 1.0000e+00],\n",
      "         [2.0000e+00, 7.2000e+01, 6.3820e-02, 4.2931e-01, 1.2764e-01,\n",
      "          1.4823e-01, 1.0000e+00],\n",
      "         [2.0000e+00, 7.2000e+01, 8.7064e-01, 4.9405e-01, 1.2711e-01,\n",
      "          1.2302e-01, 1.0000e+00],\n",
      "         [2.0000e+00, 6.2000e+01, 5.6091e-01, 5.0789e-01, 8.7500e-02,\n",
      "          1.6067e-01, 1.0000e+00],\n",
      "         [2.0000e+00, 6.2000e+01, 4.5420e-01, 5.0781e-01, 9.6609e-02,\n",
      "          1.5387e-01, 1.0000e+00],\n",
      "         [2.0000e+00, 6.2000e+01, 6.4562e-01, 5.1564e-01, 4.7141e-02,\n",
      "          1.2713e-01, 1.0000e+00],\n",
      "         [2.0000e+00, 6.2000e+01, 4.9594e-01, 5.0975e-01, 3.3719e-02,\n",
      "          1.8109e-02, 1.0000e+00],\n",
      "         [2.0000e+00, 1.0000e+00, 6.4500e-01, 4.1345e-01, 8.2891e-02,\n",
      "          2.1564e-01, 1.0000e+00],\n",
      "         [2.0000e+00, 1.0000e+00, 6.0067e-01, 4.3627e-01, 2.3625e-02,\n",
      "          5.5844e-02, 1.0000e+00],\n",
      "         [2.0000e+00, 7.8000e+01, 8.0034e-01, 4.8867e-01, 2.3031e-02,\n",
      "          2.4953e-02, 1.0000e+00],\n",
      "         [2.0000e+00, 8.2000e+01, 7.7047e-01, 4.3959e-01, 3.1703e-02,\n",
      "          1.6923e-01, 1.0000e+00],\n",
      "         [2.0000e+00, 8.4000e+01, 9.4495e-01, 6.4514e-01, 2.2406e-02,\n",
      "          7.1422e-02, 1.0000e+00],\n",
      "         [2.0000e+00, 8.4000e+01, 9.5819e-01, 6.4881e-01, 2.0125e-02,\n",
      "          7.2563e-02, 1.0000e+00],\n",
      "         [2.0000e+00, 8.5000e+01, 6.9964e-01, 3.5644e-01, 2.1828e-02,\n",
      "          3.4187e-02, 1.0000e+00],\n",
      "         [2.0000e+00, 8.6000e+01, 8.5791e-01, 6.5067e-01, 5.7313e-02,\n",
      "          1.4011e-01, 1.0000e+00],\n",
      "         [2.0000e+00, 8.6000e+01, 5.4806e-01, 4.9350e-01, 1.7766e-02,\n",
      "          3.5234e-02, 1.0000e+00],\n",
      "         [2.0000e+00, 6.2000e+01, 6.4414e-01, 5.0941e-01, 1.5047e-02,\n",
      "          1.9563e-02, 1.0000e+00],\n",
      "         [2.0000e+00, 8.6000e+01, 3.7694e-01, 4.7186e-01, 2.2219e-02,\n",
      "          2.7547e-02, 1.0000e+00],\n",
      "         [2.0000e+00, 8.6000e+01, 5.2623e-01, 4.7891e-01, 1.5203e-02,\n",
      "          2.6141e-02, 1.0000e+00],\n",
      "         [2.0000e+00, 6.7000e+01, 5.0189e-01, 5.2847e-01, 1.9619e-01,\n",
      "          1.3895e-01, 1.0000e+00],\n",
      "         [3.0000e+00, 8.8000e+01, 2.1851e-01, 2.2804e-01, 4.3702e-01,\n",
      "          4.5609e-01, 1.0000e+00],\n",
      "         [3.0000e+00, 8.8000e+01, 2.0894e-01, 4.3469e-01, 4.1788e-01,\n",
      "          5.5405e-01, 1.0000e+00],\n",
      "         [3.0000e+00, 8.8000e+01, 3.1394e-01, 2.1741e-01, 5.1911e-01,\n",
      "          4.3483e-01, 1.0000e+00],\n",
      "         [3.0000e+00, 6.5000e+01, 2.5056e-01, 2.5005e-01, 5.0112e-01,\n",
      "          5.0011e-01, 1.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 7.3000e+01, 9.9016e-02, 4.3681e-01, 1.9803e-01,\n",
      "          4.2472e-01, 2.0000e+00],\n",
      "         [0.0000e+00, 7.4000e+01, 3.7800e-01, 6.8116e-01, 1.1803e-01,\n",
      "          6.8937e-02, 2.0000e+00],\n",
      "         [0.0000e+00, 7.6000e+01, 5.0353e-01, 6.0137e-01, 4.8222e-01,\n",
      "          1.4225e-01, 2.0000e+00],\n",
      "         [0.0000e+00, 7.2000e+01, 3.9269e-01, 1.6081e-01, 3.4887e-01,\n",
      "          2.7978e-01, 2.0000e+00],\n",
      "         [0.0000e+00, 7.4000e+01, 9.5484e-01, 6.0747e-01, 4.5000e-02,\n",
      "          3.0188e-02, 2.0000e+00],\n",
      "         [1.0000e+00, 5.4000e+01, 1.9731e-01, 3.8900e-01, 3.9462e-01,\n",
      "          3.4081e-01, 2.0000e+00],\n",
      "         [1.0000e+00, 5.1000e+01, 7.6053e-01, 3.8520e-01, 2.3947e-01,\n",
      "          3.1391e-01, 2.0000e+00],\n",
      "         [2.0000e+00, 6.4000e+01, 3.7028e-01, 3.8986e-01, 3.8594e-02,\n",
      "          1.0859e-01, 2.0000e+00],\n",
      "         [2.0000e+00, 7.2000e+01, 6.3820e-02, 4.2931e-01, 1.2764e-01,\n",
      "          1.4823e-01, 2.0000e+00],\n",
      "         [2.0000e+00, 7.2000e+01, 8.7064e-01, 4.9405e-01, 1.2711e-01,\n",
      "          1.2302e-01, 2.0000e+00],\n",
      "         [2.0000e+00, 6.2000e+01, 5.6091e-01, 5.0789e-01, 8.7500e-02,\n",
      "          1.6067e-01, 2.0000e+00],\n",
      "         [2.0000e+00, 6.2000e+01, 4.5420e-01, 5.0781e-01, 9.6609e-02,\n",
      "          1.5387e-01, 2.0000e+00],\n",
      "         [2.0000e+00, 6.2000e+01, 6.4562e-01, 5.1564e-01, 4.7141e-02,\n",
      "          1.2713e-01, 2.0000e+00],\n",
      "         [2.0000e+00, 6.2000e+01, 4.9594e-01, 5.0975e-01, 3.3719e-02,\n",
      "          1.8109e-02, 2.0000e+00],\n",
      "         [2.0000e+00, 1.0000e+00, 6.4500e-01, 4.1345e-01, 8.2891e-02,\n",
      "          2.1564e-01, 2.0000e+00],\n",
      "         [2.0000e+00, 1.0000e+00, 6.0067e-01, 4.3627e-01, 2.3625e-02,\n",
      "          5.5844e-02, 2.0000e+00],\n",
      "         [2.0000e+00, 7.8000e+01, 8.0034e-01, 4.8867e-01, 2.3031e-02,\n",
      "          2.4953e-02, 2.0000e+00],\n",
      "         [2.0000e+00, 8.2000e+01, 7.7047e-01, 4.3959e-01, 3.1703e-02,\n",
      "          1.6923e-01, 2.0000e+00],\n",
      "         [2.0000e+00, 8.4000e+01, 9.4495e-01, 6.4514e-01, 2.2406e-02,\n",
      "          7.1422e-02, 2.0000e+00],\n",
      "         [2.0000e+00, 8.4000e+01, 9.5819e-01, 6.4881e-01, 2.0125e-02,\n",
      "          7.2563e-02, 2.0000e+00],\n",
      "         [2.0000e+00, 8.5000e+01, 6.9964e-01, 3.5644e-01, 2.1828e-02,\n",
      "          3.4187e-02, 2.0000e+00],\n",
      "         [2.0000e+00, 8.6000e+01, 8.5791e-01, 6.5067e-01, 5.7313e-02,\n",
      "          1.4011e-01, 2.0000e+00],\n",
      "         [2.0000e+00, 8.6000e+01, 5.4806e-01, 4.9350e-01, 1.7766e-02,\n",
      "          3.5234e-02, 2.0000e+00],\n",
      "         [2.0000e+00, 6.2000e+01, 6.4414e-01, 5.0941e-01, 1.5047e-02,\n",
      "          1.9563e-02, 2.0000e+00],\n",
      "         [2.0000e+00, 8.6000e+01, 3.7694e-01, 4.7186e-01, 2.2219e-02,\n",
      "          2.7547e-02, 2.0000e+00],\n",
      "         [2.0000e+00, 8.6000e+01, 5.2623e-01, 4.7891e-01, 1.5203e-02,\n",
      "          2.6141e-02, 2.0000e+00],\n",
      "         [2.0000e+00, 6.7000e+01, 5.0189e-01, 5.2847e-01, 1.9619e-01,\n",
      "          1.3895e-01, 2.0000e+00],\n",
      "         [3.0000e+00, 8.8000e+01, 2.1851e-01, 2.2804e-01, 4.3702e-01,\n",
      "          4.5609e-01, 2.0000e+00],\n",
      "         [3.0000e+00, 8.8000e+01, 2.0894e-01, 4.3469e-01, 4.1788e-01,\n",
      "          5.5405e-01, 2.0000e+00],\n",
      "         [3.0000e+00, 8.8000e+01, 3.1394e-01, 2.1741e-01, 5.1911e-01,\n",
      "          4.3483e-01, 2.0000e+00],\n",
      "         [3.0000e+00, 6.5000e+01, 2.5056e-01, 2.5005e-01, 5.0112e-01,\n",
      "          5.0011e-01, 2.0000e+00]]], device='cuda:0', dtype=torch.float64)\n",
      "Target format should be: (image_id, class, x, y, w, h)\n",
      "Target coordinates range: tensor(0., device='cuda:0', dtype=torch.float64) to tensor(76., device='cuda:0', dtype=torch.float64)\n",
      "Are coordinates normalized (0-1)? tensor(False, device='cuda:0')\n",
      "\n",
      "=== Layer 0 Anchor Diagnosis ===\n",
      "Anchors (scaled): tensor([[ 3.6250,  2.8125],\n",
      "        [ 4.8750,  6.1875],\n",
      "        [11.6562, 10.1875]], device='cuda:0')\n",
      "Target box sizes (sample): tensor([[2.5744, 5.5213],\n",
      "        [1.5344, 0.8962],\n",
      "        [6.2688, 1.8493],\n",
      "        [4.5354, 3.6372],\n",
      "        [0.5850, 0.3924]], device='cuda:0', dtype=torch.float64)\n",
      "Max ratios range: 1.16 to 59.59\n",
      "Anchor 0: 16/31 targets match\n",
      "Anchor 1: 13/31 targets match\n",
      "Anchor 2: 7/31 targets match\n",
      "Targets matched in layer 0: 36 out of 93\n",
      "class tensor([73, 74, 76, 72, 54, 51, 72, 72, 62, 62,  1, 67, 88, 88, 88, 65, 73, 76,\n",
      "        72, 54, 51, 72, 72, 62, 67, 88, 88, 88, 65, 72, 54, 51, 88, 88, 88, 65],\n",
      "       device='cuda:0')\n",
      "Targets matched in layer 1: 50 out of 93\n",
      "class tensor([73, 74, 51, 64, 72, 72, 62, 62, 62,  1,  1, 82, 84, 84, 86, 67, 73, 74,\n",
      "        76, 72, 74, 54, 51, 64, 72, 72, 62, 62, 62,  1, 86, 67, 73, 76, 72, 54,\n",
      "        51, 64, 72, 72, 62, 62, 62,  1, 86, 67, 88, 88, 88, 65],\n",
      "       device='cuda:0')\n",
      "Targets matched in layer 2: 52 out of 93\n",
      "class tensor([74, 64, 62,  1, 78, 84, 84, 85, 86, 62, 86, 86, 74, 74, 64, 72, 72, 62,\n",
      "        62, 62, 62,  1,  1, 78, 82, 84, 84, 85, 86, 86, 62, 86, 86, 74, 74, 64,\n",
      "        72, 72, 62, 62, 62, 62,  1,  1, 78, 82, 84, 84, 85, 86, 86, 67],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid argument\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets[:, :\u001b[38;5;241m6\u001b[39m]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# if image_id not in image_ids: continue # for when we want outputs of specific images\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m images_adv \u001b[38;5;241m=\u001b[39m \u001b[43mattacker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get adversarial image\u001b[39;00m\n\u001b[0;32m     26\u001b[0m outputsBefore \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     27\u001b[0m lossBefore, loss_components \u001b[38;5;241m=\u001b[39m compute_loss(outputsBefore, targets, model)\n",
      "File \u001b[1;32mc:\\ENG\\CS\\Master thesis\\Efficient-Robust-Object-Detection\\attacks\\CW.py:31\u001b[0m, in \u001b[0;36mCW.forward\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     29\u001b[0m x_adv\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     30\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x_adv)\n\u001b[1;32m---> 31\u001b[0m target_loss, loss_components \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# loss\u001b[39;00m\n\u001b[0;32m     32\u001b[0m l2_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39msquare(x_adv \u001b[38;5;241m-\u001b[39m x))  \u001b[38;5;66;03m# Perturbation  L2 norm\u001b[39;00m\n\u001b[0;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m target_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc \u001b[38;5;241m*\u001b[39m l2_norm \u001b[38;5;66;03m# C&W loss function\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ENG\\CS\\Master thesis\\Efficient-Robust-Object-Detection\\utils\\loss.py:124\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(predictions, targets, model)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ps\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# Hot one class encoding\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(ps[:, \u001b[38;5;241m5\u001b[39m:], device\u001b[38;5;241m=\u001b[39mdevice)  \u001b[38;5;66;03m# targets\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m     \u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_targets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtcls\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;66;03m# Use the tensor to calculate the BCE loss\u001b[39;00m\n\u001b[0;32m    126\u001b[0m     lcls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m BCEcls(ps[:, \u001b[38;5;241m5\u001b[39m:], t)  \u001b[38;5;66;03m# BCE\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: invalid argument\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "epochs = 10\n",
    "checkpoint_interval = 1\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.Adam(\n",
    "            params,\n",
    "            lr=model.hyperparams['learning_rate'],\n",
    "            weight_decay=model.hyperparams['decay'],\n",
    "        )\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    for i, (images, targets) in enumerate(tqdm(train_loader)):\n",
    "        model.train()\n",
    "        lossesEpoch = []\n",
    "        if targets[0].numel() != 0:\n",
    "            #* modify inputs to be in proper shape\n",
    "            images = torch.stack(images) # images.shape is [n, 3, 416, 416] (even if n=1)\n",
    "            images = images.to(device)\n",
    "            for i, boxes in enumerate(targets): # targets is nx6, (image,class,x,y,w,h)\n",
    "                if boxes.ndim == 2: boxes[:, 0] = i # change out image_id to id in batch to conform to compute_loss. this is normally done in ListDataset -> collate_fn\n",
    "            targets = torch.cat(targets, 0).to(device) # from tuples to one tensor\n",
    "            targets = targets[:, :6]\n",
    "            # if image_id not in image_ids: continue # for when we want outputs of specific images\n",
    "            \n",
    "            images_adv = attacker.forward(images, targets) # get adversarial image\n",
    "            outputsBefore = model(images)\n",
    "            lossBefore, loss_components = compute_loss(outputsBefore, targets, model)\n",
    "            outputsAfter = model(images_adv)\n",
    "            lossAfter, loss_components = compute_loss(outputsAfter, targets, model)\n",
    "            loss = lossBefore + lossAfter\n",
    "            lossesEpoch.append(loss.cpu().numpy())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            time.sleep(0.1) # for using noise attack\n",
    "        else: continue # pics without targets\n",
    "    losses = np.average(lossesEpoch)\n",
    "            \n",
    "    if epoch % checkpoint_interval == 0:\n",
    "        checkpoint_path = f\"./data/results/checkpoints/yolov3_ckpt_{epoch}.pth\"\n",
    "        print(f\"---- Saving checkpoint to: '{checkpoint_path}' ----\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "            \n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.15s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.08s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=9.27s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=1.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.266\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.504\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.259\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.116\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.315\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.408\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.224\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.317\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.137\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.362\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.486\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "        \n",
    "coco_gld = COCO(annFile_val) # coco\n",
    "if modelv == 2:\n",
    "    coco_rst = coco_gld.loadRes('./data/results/v2predictions.json')\n",
    "elif modelv == 3:\n",
    "    coco_rst = coco_gld.loadRes('./data/results/v3predictions.json')\n",
    "    \n",
    "coco_rst = coco_gld.loadRes('./data/results/predictionsAfter.json')\n",
    "cocoEval = COCOeval(coco_gld, coco_rst, iouType='bbox')\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "yoloenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
